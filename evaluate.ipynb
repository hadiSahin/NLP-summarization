{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"data/labeled01.csv\", encoding='cp1252')\n",
    "df2 = pd.read_csv(\"data/summaries_t5.csv\")\n",
    "df3 = pd.read_csv(\"data/summaries_bart.csv\")\n",
    "df4 = pd.read_csv(\"data/summaries_pgs.csv\")\n",
    "df = pd.DataFrame()\n",
    "df['human_summary'] = df1['summary'][0:150]\n",
    "df['t5_summary'] = df2['summary'][0:150]\n",
    "df['bart_summary'] = df3['summary'][0:150]\n",
    "df['pgs_summary'] = df4['summary'][0:150]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_summary</th>\n",
       "      <th>t5_summary</th>\n",
       "      <th>bart_summary</th>\n",
       "      <th>pgs_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Miami City Commission accepted bids from r...</td>\n",
       "      <td>miami city commission accepts bids for bio haz...</td>\n",
       "      <td>Bids received on february 18, 2021 pursuant to...</td>\n",
       "      <td>City commission meeting agenda January 13, 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Miami City Commission found that the COVID...</td>\n",
       "      <td>miami city commission finds coronavirus 2019 p...</td>\n",
       "      <td>The city commission found that the novel coron...</td>\n",
       "      <td>The commission finds that the novel coronaviru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       human_summary  \\\n",
       "0  The Miami City Commission accepted bids from r...   \n",
       "1  The Miami City Commission found that the COVID...   \n",
       "\n",
       "                                          t5_summary  \\\n",
       "0  miami city commission accepts bids for bio haz...   \n",
       "1  miami city commission finds coronavirus 2019 p...   \n",
       "\n",
       "                                        bart_summary  \\\n",
       "0  Bids received on february 18, 2021 pursuant to...   \n",
       "1  The city commission found that the novel coron...   \n",
       "\n",
       "                                         pgs_summary  \n",
       "0  City commission meeting agenda January 13, 202...  \n",
       "1  The commission finds that the novel coronaviru...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1: {'r': 0.34971456818519153, 'p': 0.2824412725619341, 'f': 0.3027437794904844}\n",
      "Average ROUGE-2: {'r': 0.17817113763707526, 'p': 0.1285662444559896, 'f': 0.14427594367596716}\n",
      "Average ROUGE-L: {'r': 0.33015373136225873, 'p': 0.2649642571221646, 'f': 0.2847871839169802}\n",
      "Average BLEU score: 0.0698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "total_rouge_1 = {'r': 0, 'p': 0, 'f': 0}\n",
    "total_rouge_2 = {'r': 0, 'p': 0, 'f': 0}\n",
    "total_rouge_l = {'r': 0, 'p': 0, 'f': 0}\n",
    "num_rows = len(df)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    rouge_scores = rouge.get_scores(row['t5_summary'], row['human_summary'])[0]\n",
    "    for metric in ['rouge-1', 'rouge-2', 'rouge-l']:\n",
    "        for score_type in ['r', 'p', 'f']:\n",
    "            if metric == 'rouge-1':\n",
    "                total_rouge_1[score_type] += rouge_scores[metric][score_type]\n",
    "            elif metric == 'rouge-2':\n",
    "                total_rouge_2[score_type] += rouge_scores[metric][score_type]\n",
    "            else:\n",
    "                total_rouge_l[score_type] += rouge_scores[metric][score_type]\n",
    "\n",
    "avg_rouge_1 = {score_type: total_rouge_1[score_type] / num_rows for score_type in ['r', 'p', 'f']}\n",
    "avg_rouge_2 = {score_type: total_rouge_2[score_type] / num_rows for score_type in ['r', 'p', 'f']}\n",
    "avg_rouge_l = {score_type: total_rouge_l[score_type] / num_rows for score_type in ['r', 'p', 'f']}\n",
    "\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "df['bleu_score'] = df.apply(lambda row: sentence_bleu([row['human_summary'].split()], row['t5_summary'].split()), axis=1)\n",
    "avg_bleu_score = df['bleu_score'].mean()\n",
    "\n",
    "\n",
    "print(f'Average ROUGE-1: {avg_rouge_1}')\n",
    "print(f'Average ROUGE-2: {avg_rouge_2}')\n",
    "print(f'Average ROUGE-L: {avg_rouge_l}')\n",
    "print(f'Average BLEU score: {avg_bleu_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1: {'r': 0.39058341843081357, 'p': 0.2951865067673829, 'f': 0.3269490769023268}\n",
      "Average ROUGE-2: {'r': 0.20322398369646003, 'p': 0.14357114338755977, 'f': 0.1621193815429138}\n",
      "Average ROUGE-L: {'r': 0.3601535004659526, 'p': 0.27199057785140773, 'f': 0.30123608929090095}\n",
      "Average BLEU score: 0.0924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# rouge = Rouge()\n",
    "# rouge_scores_bart = [rouge.get_scores(row['bart_summary'], row['human_summary']) for _, row in df.iterrows()]\n",
    "\n",
    "# # prepare data for bleu calculation\n",
    "# nlp_summaries_lists = [[summary.split()] for summary in df['bart_summary']]\n",
    "# human_summaries_lists = [summary.split() for summary in df['human_summary']]\n",
    "# bleu_score_bart = corpus_bleu(nlp_summaries_lists, human_summaries_lists)\n",
    "\n",
    "# print('ROUGE scores:', rouge_scores_bart)\n",
    "# print('BLEU score:', bleu_score_bart)\n",
    "\n",
    "\n",
    "rouge = Rouge()\n",
    "total_rouge_1 = {'r': 0, 'p': 0, 'f': 0}\n",
    "total_rouge_2 = {'r': 0, 'p': 0, 'f': 0}\n",
    "total_rouge_l = {'r': 0, 'p': 0, 'f': 0}\n",
    "num_rows = len(df)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    rouge_scores = rouge.get_scores(row['bart_summary'], row['human_summary'])[0]\n",
    "    for metric in ['rouge-1', 'rouge-2', 'rouge-l']:\n",
    "        for score_type in ['r', 'p', 'f']:\n",
    "            if metric == 'rouge-1':\n",
    "                total_rouge_1[score_type] += rouge_scores[metric][score_type]\n",
    "            elif metric == 'rouge-2':\n",
    "                total_rouge_2[score_type] += rouge_scores[metric][score_type]\n",
    "            else:\n",
    "                total_rouge_l[score_type] += rouge_scores[metric][score_type]\n",
    "\n",
    "avg_rouge_1 = {score_type: total_rouge_1[score_type] / num_rows for score_type in ['r', 'p', 'f']}\n",
    "avg_rouge_2 = {score_type: total_rouge_2[score_type] / num_rows for score_type in ['r', 'p', 'f']}\n",
    "avg_rouge_l = {score_type: total_rouge_l[score_type] / num_rows for score_type in ['r', 'p', 'f']}\n",
    "\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "df['bleu_score'] = df.apply(lambda row: sentence_bleu([row['human_summary'].split()], row['bart_summary'].split()), axis=1)\n",
    "avg_bleu_score = df['bleu_score'].mean()\n",
    "\n",
    "\n",
    "print(f'Average ROUGE-1: {avg_rouge_1}')\n",
    "print(f'Average ROUGE-2: {avg_rouge_2}')\n",
    "print(f'Average ROUGE-L: {avg_rouge_l}')\n",
    "print(f'Average BLEU score: {avg_bleu_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1: {'r': 0.3764299453576587, 'p': 0.2627328420204445, 'f': 0.3019300683261253}\n",
      "Average ROUGE-2: {'r': 0.184709117630587, 'p': 0.11991324221889106, 'f': 0.1411673781828742}\n",
      "Average ROUGE-L: {'r': 0.34935435693012534, 'p': 0.2434059862425279, 'f': 0.2799091869075838}\n",
      "Average BLEU score: 0.0757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# rouge = Rouge()\n",
    "# rouge_scores_pgs = [rouge.get_scores(row['pgs_summary'], row['human_summary']) for _, row in df.iterrows()]\n",
    "\n",
    "# # prepare data for bleu calculation\n",
    "# nlp_summaries_lists = [[summary.split()] for summary in df['pgs_summary']]\n",
    "# human_summaries_lists = [summary.split() for summary in df['human_summary']]\n",
    "# bleu_score_pgs = corpus_bleu(nlp_summaries_lists, human_summaries_lists)\n",
    "\n",
    "# print('ROUGE scores:', rouge_scores_pgs)\n",
    "# print('BLEU score:', bleu_score_pgs)\n",
    "\n",
    "\n",
    "rouge = Rouge()\n",
    "total_rouge_1 = {'r': 0, 'p': 0, 'f': 0}\n",
    "total_rouge_2 = {'r': 0, 'p': 0, 'f': 0}\n",
    "total_rouge_l = {'r': 0, 'p': 0, 'f': 0}\n",
    "num_rows = len(df)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    rouge_scores = rouge.get_scores(row['pgs_summary'], row['human_summary'])[0]\n",
    "    for metric in ['rouge-1', 'rouge-2', 'rouge-l']:\n",
    "        for score_type in ['r', 'p', 'f']:\n",
    "            if metric == 'rouge-1':\n",
    "                total_rouge_1[score_type] += rouge_scores[metric][score_type]\n",
    "            elif metric == 'rouge-2':\n",
    "                total_rouge_2[score_type] += rouge_scores[metric][score_type]\n",
    "            else:\n",
    "                total_rouge_l[score_type] += rouge_scores[metric][score_type]\n",
    "\n",
    "avg_rouge_1 = {score_type: total_rouge_1[score_type] / num_rows for score_type in ['r', 'p', 'f']}\n",
    "avg_rouge_2 = {score_type: total_rouge_2[score_type] / num_rows for score_type in ['r', 'p', 'f']}\n",
    "avg_rouge_l = {score_type: total_rouge_l[score_type] / num_rows for score_type in ['r', 'p', 'f']}\n",
    "\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "df['bleu_score'] = df.apply(lambda row: sentence_bleu([row['human_summary'].split()], row['pgs_summary'].split()), axis=1)\n",
    "avg_bleu_score = df['bleu_score'].mean()\n",
    "\n",
    "\n",
    "print(f'Average ROUGE-1: {avg_rouge_1}')\n",
    "print(f'Average ROUGE-2: {avg_rouge_2}')\n",
    "print(f'Average ROUGE-L: {avg_rouge_l}')\n",
    "print(f'Average BLEU score: {avg_bleu_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "model = BartForConditionalGeneration.from_pretrained(\"./results2/checkpoint-500\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Define a function to generate summaries\n",
    "def generate_summary(text, model, tokenizer):\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(input_ids, do_sample=True, max_new_tokens=50, min_length=10)\n",
    "    \n",
    "    # Decode the summary\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# # Example usage\n",
    "# text = \"resolution miami city commission accepting bids received february pursuant invitation bid responsive responsible bidders bioresponse corp florida profit corporation jrp sons corporation fl cleanup florida profit corporation muma sa lc florida limited liability company ifb group pre qualified pool vendors provision bio hazardous waste spills decontamination services needed basis citywide initial term two years option renew two additional two year periods allocating funds end user departments sources funds subject availability funds budgetary approval time need authorizing city manager negotiate execute documents including amendments renewals extensions subject allocations appropriations prior budgetary approvals compliance applicable provisions code city miami florida amended including city procurement ordinance anti deficiency act financial integrity principles set forth chapter city code forms acceptable city attorney compliance applicable laws rules regulations may deemed necessary said purpose city commission meeting agenda january city miami page printed ca department real estate asset management resolution.\"\n",
    "# summary = generate_summary(text, model, tokenizer)\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text,num):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text #remove links\n",
    "    # newString = re.sub(r'\\([^)]*\\)', '', newString) # remove text in the paranthesis\n",
    "    newString = re.sub('_','', newString) # removing underscores\n",
    "    newString = re.sub('\"','', newString) # removing double quotes\n",
    "    newString = re.sub('/',' ', newString) # removing double quotes\n",
    "    newString = re.sub('-','', newString) # removing double quotes\n",
    "    newString = re.sub(r'[^\\x00-\\x7F]+', '', newString) # removes non-ASCII characters\n",
    "    # newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString) #remove possesive s\n",
    "    # newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
    "    # if(num==0):\n",
    "    #     tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    # else:\n",
    "    #     tokens=newString.split()\n",
    "    tokens=newString.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                                 #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>resolution</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resolution of the miami city commission accept...</td>\n",
       "      <td>The Miami City Commission accepted bids from r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resolution of the miami city commission findin...</td>\n",
       "      <td>The Miami City Commission found that the COVID...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>resolution of the miami city commission author...</td>\n",
       "      <td>Miami City Commission authorizes city manager ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>resolution of the miami city commission, with ...</td>\n",
       "      <td>Miami City Commission accepts perpetual sidewa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>resolution of the miami city commission, with ...</td>\n",
       "      <td>Miami City Commission accepts two right-of-way...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          resolution  \\\n",
       "0  resolution of the miami city commission accept...   \n",
       "1  resolution of the miami city commission findin...   \n",
       "2  resolution of the miami city commission author...   \n",
       "3  resolution of the miami city commission, with ...   \n",
       "4  resolution of the miami city commission, with ...   \n",
       "\n",
       "                                             summary  \n",
       "0  The Miami City Commission accepted bids from r...  \n",
       "1  The Miami City Commission found that the COVID...  \n",
       "2  Miami City Commission authorizes city manager ...  \n",
       "3  Miami City Commission accepts perpetual sidewa...  \n",
       "4  Miami City Commission accepts two right-of-way...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df1.iloc[0:26]\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the summaries\n",
    "summaries = []\n",
    "\n",
    "for text in df_test['resolution']:\n",
    "    summary = generate_summary(text_cleaner(text,0), model, tokenizer)\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Create a new DataFrame with the summaries\n",
    "summary_df = pd.DataFrame({'summary': summaries})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "# summary_df.to_csv('summaries.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Miami city commission accepts bids receive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Miami city commission finds that the novel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary\n",
       "0  The Miami city commission accepts bids receive...\n",
       "1  The Miami city commission finds that the novel..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bart_n_summary'] = summary_df['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_summary</th>\n",
       "      <th>t5_summary</th>\n",
       "      <th>bart_summary</th>\n",
       "      <th>pgs_summary</th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>bart_n_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Miami City Commission accepted bids from r...</td>\n",
       "      <td>miami city commission accepts bids for bio haz...</td>\n",
       "      <td>Bids received on february 18, 2021 pursuant to...</td>\n",
       "      <td>City commission meeting agenda January 13, 202...</td>\n",
       "      <td>1.201524e-231</td>\n",
       "      <td>The Miami city commission accepts bids receive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Miami City Commission found that the COVID...</td>\n",
       "      <td>miami city commission finds coronavirus 2019 p...</td>\n",
       "      <td>The city commission found that the novel coron...</td>\n",
       "      <td>The commission finds that the novel coronaviru...</td>\n",
       "      <td>4.955625e-155</td>\n",
       "      <td>The Miami city commission finds that the novel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       human_summary  \\\n",
       "0  The Miami City Commission accepted bids from r...   \n",
       "1  The Miami City Commission found that the COVID...   \n",
       "\n",
       "                                          t5_summary  \\\n",
       "0  miami city commission accepts bids for bio haz...   \n",
       "1  miami city commission finds coronavirus 2019 p...   \n",
       "\n",
       "                                        bart_summary  \\\n",
       "0  Bids received on february 18, 2021 pursuant to...   \n",
       "1  The city commission found that the novel coron...   \n",
       "\n",
       "                                         pgs_summary     bleu_score  \\\n",
       "0  City commission meeting agenda January 13, 202...  1.201524e-231   \n",
       "1  The commission finds that the novel coronaviru...  4.955625e-155   \n",
       "\n",
       "                                      bart_n_summary  \n",
       "0  The Miami city commission accepts bids receive...  \n",
       "1  The Miami city commission finds that the novel...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Administrator\\OneDrive - Florida International University\\Documents\\ECE Spring'23\\NLP\\proposal\\code\\evaluate.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/OneDrive%20-%20Florida%20International%20University/Documents/ECE%20Spring%2723/NLP/proposal/code/evaluate.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rouge \u001b[39m=\u001b[39m Rouge()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Administrator/OneDrive%20-%20Florida%20International%20University/Documents/ECE%20Spring%2723/NLP/proposal/code/evaluate.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m rouge_scores_pgs \u001b[39m=\u001b[39m [rouge\u001b[39m.\u001b[39mget_scores(row[\u001b[39m'\u001b[39m\u001b[39mbart_n_summary\u001b[39m\u001b[39m'\u001b[39m], row[\u001b[39m'\u001b[39m\u001b[39mhuman_summary\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39mfor\u001b[39;00m _, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows()]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/OneDrive%20-%20Florida%20International%20University/Documents/ECE%20Spring%2723/NLP/proposal/code/evaluate.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# prepare data for bleu calculation\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/OneDrive%20-%20Florida%20International%20University/Documents/ECE%20Spring%2723/NLP/proposal/code/evaluate.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m nlp_summaries_lists \u001b[39m=\u001b[39m [[summary\u001b[39m.\u001b[39msplit()] \u001b[39mfor\u001b[39;00m summary \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mbart_n_summary\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "\u001b[1;32mc:\\Users\\Administrator\\OneDrive - Florida International University\\Documents\\ECE Spring'23\\NLP\\proposal\\code\\evaluate.ipynb Cell 14\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/OneDrive%20-%20Florida%20International%20University/Documents/ECE%20Spring%2723/NLP/proposal/code/evaluate.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rouge \u001b[39m=\u001b[39m Rouge()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Administrator/OneDrive%20-%20Florida%20International%20University/Documents/ECE%20Spring%2723/NLP/proposal/code/evaluate.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m rouge_scores_pgs \u001b[39m=\u001b[39m [rouge\u001b[39m.\u001b[39;49mget_scores(row[\u001b[39m'\u001b[39;49m\u001b[39mbart_n_summary\u001b[39;49m\u001b[39m'\u001b[39;49m], row[\u001b[39m'\u001b[39;49m\u001b[39mhuman_summary\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39mfor\u001b[39;00m _, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows()]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/OneDrive%20-%20Florida%20International%20University/Documents/ECE%20Spring%2723/NLP/proposal/code/evaluate.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# prepare data for bleu calculation\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Administrator/OneDrive%20-%20Florida%20International%20University/Documents/ECE%20Spring%2723/NLP/proposal/code/evaluate.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m nlp_summaries_lists \u001b[39m=\u001b[39m [[summary\u001b[39m.\u001b[39msplit()] \u001b[39mfor\u001b[39;00m summary \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mbart_n_summary\u001b[39m\u001b[39m'\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rouge\\rouge.py:103\u001b[0m, in \u001b[0;36mRouge.get_scores\u001b[1;34m(self, hyps, refs, avg, ignore_empty)\u001b[0m\n\u001b[0;32m     98\u001b[0m     hyps_and_refs \u001b[39m=\u001b[39m [_ \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m hyps_and_refs\n\u001b[0;32m     99\u001b[0m                      \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_[\u001b[39m0\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    100\u001b[0m                      \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(_[\u001b[39m1\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[0;32m    101\u001b[0m     hyps, refs \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mhyps_and_refs)\n\u001b[1;32m--> 103\u001b[0m \u001b[39massert\u001b[39;00m(\u001b[39misinstance\u001b[39m(hyps, \u001b[39mtype\u001b[39m(refs)))\n\u001b[0;32m    104\u001b[0m \u001b[39massert\u001b[39;00m(\u001b[39mlen\u001b[39m(hyps) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(refs))\n\u001b[0;32m    106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m avg:\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rouge = Rouge()\n",
    "rouge_scores_pgs = [rouge.get_scores(row['bart_n_summary'], row['human_summary']) for _, row in df.iterrows()]\n",
    "\n",
    "# prepare data for bleu calculation\n",
    "nlp_summaries_lists = [[summary.split()] for summary in df['bart_n_summary']]\n",
    "human_summaries_lists = [summary.split() for summary in df['human_summary']]\n",
    "bleu_score_pgs = corpus_bleu(nlp_summaries_lists, human_summaries_lists)\n",
    "\n",
    "print('ROUGE scores:', rouge_scores_pgs)\n",
    "print('BLEU score:', bleu_score_pgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
